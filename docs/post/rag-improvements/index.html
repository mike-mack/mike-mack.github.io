<!DOCTYPE html>
<html lang="en-us"><head><meta charset="utf-8">
<meta http-equiv="content-type" content="text/html">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<title itemprop="name">RAG Improvements | </title>
<meta property="og:title" content="RAG Improvements | " />
<meta name="twitter:title" content="RAG Improvements | " />
<meta itemprop="name" content="RAG Improvements | " />
<meta name="application-name" content="RAG Improvements | " />
<meta property="og:site_name" content="" />

<meta name="description" content="">
<meta itemprop="description" content="" />
<meta property="og:description" content="" />
<meta name="twitter:description" content="" />

<meta property="og:locale" content="en-us" />
<meta name="language" content="en-us" />

  <link rel="alternate" hreflang="en" href="../../post/rag-improvements/" title="" />






<meta name="generator" content="Hugo 0.152.1">

    
    <meta property="og:url" content="/post/rag-improvements/">
  <meta property="og:title" content="RAG Improvements">
  <meta property="og:description" content="A Basic Pipeline A Retrieval-Augmented Generation (RAG) pipeline consists of four main parts: ingestion, storage, retrieval, and generation. To optimize it, start with a simple local setup using LangChain 1.0 and lightweight models. This baseline will serve as the foundation for all further experiments.
The goal is to build a fully local RAG pipeline with a local embedding model, a vector database (FAISS or Chroma), a local LLM such as Ollama, and a small text dataset. This setup lets you measure latency, memory use, and context quality, debug each component independently, and verify end-to-end ingestion and retrieval.">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="post">
    <meta property="article:published_time" content="2025-10-25T16:29:51-04:00">
    <meta property="article:modified_time" content="2025-10-25T16:29:51-04:00">


    
    
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="RAG Improvements">
  <meta name="twitter:description" content="A Basic Pipeline A Retrieval-Augmented Generation (RAG) pipeline consists of four main parts: ingestion, storage, retrieval, and generation. To optimize it, start with a simple local setup using LangChain 1.0 and lightweight models. This baseline will serve as the foundation for all further experiments.
The goal is to build a fully local RAG pipeline with a local embedding model, a vector database (FAISS or Chroma), a local LLM such as Ollama, and a small text dataset. This setup lets you measure latency, memory use, and context quality, debug each component independently, and verify end-to-end ingestion and retrieval.">


    

    <link rel="canonical" href="../../post/rag-improvements/">
    <link href="../../style.min.e390ba7da26222f4dc42a349955d76dbbe44e5ce2535a43de5a70633a0a9ec3c.css" rel="stylesheet">
    <link href="../../code-highlight.min.706d31975fec544a864cb7f0d847a73ea55ca1df91bf495fd12a177138d807cf.css" rel="stylesheet">

    
    <link rel="apple-touch-icon" sizes="180x180" href="../../icons/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="../../icons/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../../icons/favicon-16x16.png">
    <link rel="mask-icon" href="../../icons/safari-pinned-tab.svg">
    <link rel="shortcut icon" href="../../favicon.ico">




<link rel="manifest" href="../../site.webmanifest">

<meta name="msapplication-config" content="/browserconfig.xml">
<meta name="msapplication-TileColor" content="#2d89ef">
<meta name="theme-color" content="#434648">
    <meta name="color-scheme" content="light dark">

    
    <link rel="icon" type="image/svg+xml" href="../../icons/favicon.svg">

    
    
    
</head>
<body data-theme = "" class="notransition">

<script src="../../js/theme.min.8961c317c5b88b953fe27525839672c9343f1058ab044696ca225656c8ba2ab0.js" integrity="sha256-iWHDF8W4i5U/4nUlg5ZyyTQ/EFirBEaWyiJWVsi6KrA="></script>

<div class="navbar" role="navigation">
    <nav class="menu" aria-label="Main Navigation">
        <a href="../../" class="logo">
            <svg xmlns="http://www.w3.org/2000/svg" width="25" height="25" 
viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" 
stroke-linejoin="round" class="feather feather-home">
<title></title>
<path d="M3 9l9-7 9 7v11a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2z"></path>
<polyline points="9 22 9 12 15 12 15 22"></polyline>
</svg>
        </a>
        <input type="checkbox" id="menu-trigger" class="menu-trigger" />
        <label for="menu-trigger">
            <span class="menu-icon">
                <svg xmlns="http://www.w3.org/2000/svg" width="25" height="25" stroke="currentColor" fill="none" viewBox="0 0 14 14"><title>Menu</title><path stroke-linecap="round" stroke-linejoin="round" d="M10.595 7L3.40726 7"></path><path stroke-linecap="round" stroke-linejoin="round" d="M10.5096 3.51488L3.49301 3.51488"></path><path stroke-linecap="round" stroke-linejoin="round" d="M10.5096 10.4851H3.49301"></path><path stroke-linecap="round" stroke-linejoin="round" d="M0.5 12.5V1.5C0.5 0.947715 0.947715 0.5 1.5 0.5H12.5C13.0523 0.5 13.5 0.947715 13.5 1.5V12.5C13.5 13.0523 13.0523 13.5 12.5 13.5H1.5C0.947715 13.5 0.5 13.0523 0.5 12.5Z"></path></svg>
            </span>
        </label>

        <div class="trigger">
            <ul class="trigger-container">
                
                
                <li class="menu-separator">
                    <span>|</span>
                </li>
                
                
            </ul>
            <a id="mode" href="#">
                <svg xmlns="http://www.w3.org/2000/svg" class="mode-sunny" width="21" height="21" viewBox="0 0 14 14" stroke-width="1">
<title>LIGHT</title><g><circle cx="7" cy="7" r="2.5" fill="none" stroke-linecap="round" stroke-linejoin="round"></circle><line x1="7" y1="0.5" x2="7" y2="2.5" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="2.4" y1="2.4" x2="3.82" y2="3.82" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="0.5" y1="7" x2="2.5" y2="7" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="2.4" y1="11.6" x2="3.82" y2="10.18" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="7" y1="13.5" x2="7" y2="11.5" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="11.6" y1="11.6" x2="10.18" y2="10.18" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="13.5" y1="7" x2="11.5" y2="7" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="11.6" y1="2.4" x2="10.18" y2="3.82" fill="none" stroke-linecap="round" stroke-linejoin="round"></line></g></svg>
                <svg xmlns="http://www.w3.org/2000/svg" class="mode-moon" width="21" height="21" viewBox="0 0 14 14" stroke-width="1">
<title>DARK</title><g><circle cx="7" cy="7" r="2.5" fill="none" stroke-linecap="round" stroke-linejoin="round"></circle><line x1="7" y1="0.5" x2="7" y2="2.5" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="2.4" y1="2.4" x2="3.82" y2="3.82" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="0.5" y1="7" x2="2.5" y2="7" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="2.4" y1="11.6" x2="3.82" y2="10.18" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="7" y1="13.5" x2="7" y2="11.5" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="11.6" y1="11.6" x2="10.18" y2="10.18" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="13.5" y1="7" x2="11.5" y2="7" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="11.6" y1="2.4" x2="10.18" y2="3.82" fill="none" stroke-linecap="round" stroke-linejoin="round"></line></g></svg>
            </a>
        </div>
    </nav>
</div>

<div class="wrapper post">
    <main class="page-content" aria-label="Content">
        <article>
            <header class="header">
                <h1 class="header-title">RAG Improvements</h1>
                
                
                
                <div class="post-meta">
                    <time datetime="2025-10-25T16:29:51-04:00" itemprop="datePublished"> Oct 25, 2025 </time>
                </div>
                
            </header>
            
            <div class="page-content">
                <h2 id="a-basic-pipeline">A Basic Pipeline</h2>
<p>A Retrieval-Augmented Generation (RAG) pipeline consists of four main parts: ingestion, storage, retrieval, and generation. To optimize it, start with a simple local setup using LangChain 1.0 and lightweight models. This baseline will serve as the foundation for all further experiments.</p>
<p>The goal is to build a fully local RAG pipeline with a local embedding model, a vector database (FAISS or Chroma), a local LLM such as Ollama, and a small text dataset. This setup lets you measure latency, memory use, and context quality, debug each component independently, and verify end-to-end ingestion and retrieval.</p>
<p>The pipeline works as follows. First, load documents from local files using a LangChain document loader. Next, split the text into fixed-size chunks with no overlap. Convert each chunk into a dense vector using the embedding model. Store these vectors in a vector database. For queries, retrieve the top-K most similar chunks and feed them to a local LLM to generate answers.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-Python" data-lang="Python"><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain_community.document_loaders <span style="color:#f92672">import</span> TextLoader
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain.text_splitter <span style="color:#f92672">import</span> RecursiveCharacterTextSplitter
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain_community.vectorstores <span style="color:#f92672">import</span> FAISS
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain_community.embeddings <span style="color:#f92672">import</span> SentenceTransformerEmbeddings
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain_community.llms <span style="color:#f92672">import</span> Ollama
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain.chains <span style="color:#f92672">import</span> RetrievalQA
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Load documents</span>
</span></span><span style="display:flex;"><span>loader <span style="color:#f92672">=</span> TextLoader(<span style="color:#e6db74">&#34;docs/sample.txt&#34;</span>)
</span></span><span style="display:flex;"><span>documents <span style="color:#f92672">=</span> loader<span style="color:#f92672">.</span>load()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Split into chunks</span>
</span></span><span style="display:flex;"><span>splitter <span style="color:#f92672">=</span> RecursiveCharacterTextSplitter(chunk_size<span style="color:#f92672">=</span><span style="color:#ae81ff">800</span>, chunk_overlap<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>chunks <span style="color:#f92672">=</span> splitter<span style="color:#f92672">.</span>split_documents(documents)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Create embeddings</span>
</span></span><span style="display:flex;"><span>embedding_model <span style="color:#f92672">=</span> SentenceTransformerEmbeddings(model_name<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;all-MiniLM-L6-v2&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Build vector store</span>
</span></span><span style="display:flex;"><span>vectorstore <span style="color:#f92672">=</span> FAISS<span style="color:#f92672">.</span>from_documents(chunks, embedding_model)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Create retriever</span>
</span></span><span style="display:flex;"><span>retriever <span style="color:#f92672">=</span> vectorstore<span style="color:#f92672">.</span>as_retriever(search_kwargs<span style="color:#f92672">=</span>{<span style="color:#e6db74">&#34;k&#34;</span>: <span style="color:#ae81ff">4</span>})
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Local LLM</span>
</span></span><span style="display:flex;"><span>llm <span style="color:#f92672">=</span> Ollama(model<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;mistral&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Construct retrieval-augmented chain</span>
</span></span><span style="display:flex;"><span>qa_chain <span style="color:#f92672">=</span> RetrievalQA<span style="color:#f92672">.</span>from_chain_type(
</span></span><span style="display:flex;"><span>    llm<span style="color:#f92672">=</span>llm,
</span></span><span style="display:flex;"><span>    chain_type<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;stuff&#34;</span>,
</span></span><span style="display:flex;"><span>    retriever<span style="color:#f92672">=</span>retriever,
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Run a query</span>
</span></span><span style="display:flex;"><span>query <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;Summarize the main argument in the document.&#34;</span>
</span></span><span style="display:flex;"><span>response <span style="color:#f92672">=</span> qa_chain<span style="color:#f92672">.</span>invoke(query)
</span></span><span style="display:flex;"><span>print(response[<span style="color:#e6db74">&#34;result&#34;</span>])
</span></span></code></pre></div><p>This baseline covers the essential RAG workflow. Ingestion is straightforward, using text only with no metadata. Retrieval relies on vector-based cosine similarity with default settings. Generation uses simple context stuffing without prompt optimization. You can benchmark this setup for end-to-end latency, recall quality, and embedding throughput. These metrics establish a reference point for improvements such as advanced chunking strategies, metadata usage, and refined retrieval logic. The following section will explore recursive, semantic, and sliding-window chunking to enhance recall and reduce context fragmentation.</p>
<h2 id="hierarchical-retrieval">Hierarchical Retrieval</h2>
<p>Flat retrieval systems do not scale well. As the dataset grows, similarity search becomes slow and returns irrelevant results. More hardware does not fix the problem; better structure does. Hierarchical retrieval adds stages to the search process. The first stage finds broad areas of relevance. The second stage searches within those areas for finer details.</p>
<p>The first stage retrieves large chunks such as full documents or long sections. These provide coverage of general topics. The second stage searches within those top results using smaller chunks, such as paragraphs or sentences, to find precise matches. This approach reduces the number of comparisons and improves accuracy by discarding irrelevant material early.</p>
<p>The method balances precision and recall. Large chunks capture more context but are less specific. Small chunks are more specific but can lose context. Two-stage retrieval combines both. It is also faster: instead of searching every sub-chunk in a large corpus, the first stage narrows the candidates to a small set. This is useful for technical or multi-domain data where high-level grouping matters.</p>
<p>The following example shows a two-level retriever built with LangChain 1.0 and FAISS. The first retriever indexes full documents. The second indexes smaller parts within the top results.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-Python" data-lang="Python"><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain_community.document_loaders <span style="color:#f92672">import</span> TextLoader
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain.text_splitter <span style="color:#f92672">import</span> RecursiveCharacterTextSplitter
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain_community.embeddings <span style="color:#f92672">import</span> SentenceTransformerEmbeddings
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain_community.vectorstores <span style="color:#f92672">import</span> FAISS
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain_community.llms <span style="color:#f92672">import</span> Ollama
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Stage 1: Coarse retrieval</span>
</span></span><span style="display:flex;"><span>coarse_loader <span style="color:#f92672">=</span> TextLoader(<span style="color:#e6db74">&#34;docs/knowledge_base.txt&#34;</span>)
</span></span><span style="display:flex;"><span>coarse_docs <span style="color:#f92672">=</span> coarse_loader<span style="color:#f92672">.</span>load()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>coarse_splitter <span style="color:#f92672">=</span> RecursiveCharacterTextSplitter(chunk_size<span style="color:#f92672">=</span><span style="color:#ae81ff">4000</span>, chunk_overlap<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>coarse_chunks <span style="color:#f92672">=</span> coarse_splitter<span style="color:#f92672">.</span>split_documents(coarse_docs)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>embedding_model <span style="color:#f92672">=</span> SentenceTransformerEmbeddings(model_name<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;all-MiniLM-L6-v2&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>coarse_store <span style="color:#f92672">=</span> FAISS<span style="color:#f92672">.</span>from_documents(coarse_chunks, embedding_model)
</span></span><span style="display:flex;"><span>coarse_retriever <span style="color:#f92672">=</span> coarse_store<span style="color:#f92672">.</span>as_retriever(search_kwargs<span style="color:#f92672">=</span>{<span style="color:#e6db74">&#34;k&#34;</span>: <span style="color:#ae81ff">5</span>})
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Stage 2: Fine retrieval</span>
</span></span><span style="display:flex;"><span>fine_splitter <span style="color:#f92672">=</span> RecursiveCharacterTextSplitter(chunk_size<span style="color:#f92672">=</span><span style="color:#ae81ff">800</span>, chunk_overlap<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">hierarchical_retrieve</span>(query: str):
</span></span><span style="display:flex;"><span>    coarse_results <span style="color:#f92672">=</span> coarse_retriever<span style="color:#f92672">.</span>get_relevant_documents(query)
</span></span><span style="display:flex;"><span>    fine_docs <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> doc <span style="color:#f92672">in</span> coarse_results:
</span></span><span style="display:flex;"><span>        fine_docs<span style="color:#f92672">.</span>extend(fine_splitter<span style="color:#f92672">.</span>split_text(doc<span style="color:#f92672">.</span>page_content))
</span></span><span style="display:flex;"><span>    fine_store <span style="color:#f92672">=</span> FAISS<span style="color:#f92672">.</span>from_texts(fine_docs, embedding_model)
</span></span><span style="display:flex;"><span>    fine_retriever <span style="color:#f92672">=</span> fine_store<span style="color:#f92672">.</span>as_retriever(search_kwargs<span style="color:#f92672">=</span>{<span style="color:#e6db74">&#34;k&#34;</span>: <span style="color:#ae81ff">4</span>})
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> fine_retriever<span style="color:#f92672">.</span>get_relevant_documents(query)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Stage 3: Response generation</span>
</span></span><span style="display:flex;"><span>llm <span style="color:#f92672">=</span> Ollama(model<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;mistral&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">answer_query</span>(query):
</span></span><span style="display:flex;"><span>    fine_chunks <span style="color:#f92672">=</span> hierarchical_retrieve(query)
</span></span><span style="display:flex;"><span>    context <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\n\n</span><span style="color:#e6db74">&#34;</span><span style="color:#f92672">.</span>join([chunk<span style="color:#f92672">.</span>page_content <span style="color:#66d9ef">for</span> chunk <span style="color:#f92672">in</span> fine_chunks])
</span></span><span style="display:flex;"><span>    prompt <span style="color:#f92672">=</span> <span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Use the following context to answer:</span><span style="color:#ae81ff">\n\n</span><span style="color:#e6db74">{</span>context<span style="color:#e6db74">}</span><span style="color:#ae81ff">\n\n</span><span style="color:#e6db74">Question: </span><span style="color:#e6db74">{</span>query<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> llm<span style="color:#f92672">.</span>invoke(prompt)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>response <span style="color:#f92672">=</span> answer_query(<span style="color:#e6db74">&#34;How does the cache invalidation logic work?&#34;</span>)
</span></span><span style="display:flex;"><span>print(response)
</span></span></code></pre></div><p>For better ranking, use a CrossEncoder to score the top results. It evaluates query and document pairs together for higher accuracy, at the cost of more computation.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-Python" data-lang="Python"><span style="display:flex;"><span><span style="color:#f92672">from</span> sentence_transformers <span style="color:#f92672">import</span> CrossEncoder
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>reranker <span style="color:#f92672">=</span> CrossEncoder(<span style="color:#e6db74">&#34;cross-encoder/ms-marco-MiniLM-L-6-v2&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">rerank_chunks</span>(query, docs, top_k<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>):
</span></span><span style="display:flex;"><span>    pairs <span style="color:#f92672">=</span> [(query, d<span style="color:#f92672">.</span>page_content) <span style="color:#66d9ef">for</span> d <span style="color:#f92672">in</span> docs]
</span></span><span style="display:flex;"><span>    scores <span style="color:#f92672">=</span> reranker<span style="color:#f92672">.</span>predict(pairs)
</span></span><span style="display:flex;"><span>    ranked <span style="color:#f92672">=</span> sorted(zip(docs, scores), key<span style="color:#f92672">=</span><span style="color:#66d9ef">lambda</span> x: x[<span style="color:#ae81ff">1</span>], reverse<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> [doc <span style="color:#66d9ef">for</span> doc, _ <span style="color:#f92672">in</span> ranked[:top_k]]
</span></span></code></pre></div><p>This combination—FAISS for filtering and a CrossEncoder for re-ranking—offers high precision while keeping runtime low.</p>
<p>Compared to flat retrieval, hierarchical retrieval is faster and more accurate. Latency drops sharply for large datasets. Top-answer relevance improves by roughly 10–20%. Storage costs decrease because only a small subset of text is embedded at fine granularity. For large document collections, this approach gives better performance and higher-quality answers than any single-stage retriever.</p>

            </div>
        </article></main>
</div>
<footer class="footer">
    <span class="footer_item"> </span>
    &nbsp;

    <div class="footer_social-icons">
</div>
    <small class="footer_copyright">
        © 2025 .
        
    </small>
</footer>







    
    <script src="../../js/main.min.4ee188e1744c19816e95a540b2650ed9f033ea0371e74eac8e717355cfca8741.js" integrity="sha256-TuGI4XRMGYFulaVAsmUO2fAz6gNx506sjnFzVc/Kh0E="></script>

    

</body>
</html>
